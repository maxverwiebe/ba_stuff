\chapter{Baseline Clustering with k-Means}
To solve these problems, as a first step, we want to find $k$ clusters in our dataset, which are going to be improved and optimized by our optimization clustering algorithm in the next section.
To create this fixed amount of clusters, we are using the standard $k$-Means algorithm introduced in Chapter 2.

\section*{Selection of Parameters}
To manipulate the behavior of the algorithm, we have to chose its parameters.

\begin{itemize}
  \item \textbf{Number of clusters $k$.} We choose an initial $k_0$ from storage constraints,
  \[
  k_0 \;=\; \left\lceil \frac{N}{\textit{target\_leaf\_size}}\right\rceil,\qquad
  \textit{target\_leaf\_size}\approx \text{MAX\_POINTS},
  \]
  so that the generated clusters are approximately going to fit into one leaf (one data page).
  \item \textbf{Initialization.} $k$-Means
  \item \textbf{Distance / scaling.} Squared Euclidean distance
  \item \textbf{Stopping.} Max iterations: 300
  \item \textbf{Reproducibility.} Fixed random seed per test case
\end{itemize}


\section*{Analysis of Resulting Clusters}
$k$-Means generates some very typical clusters.

\paragraph{Shape} The shape of the clusters generated by $k$-Means is convex. In lower dimensions, the resulting clusters are therefore more spherical or hyperspherical in higher dimensions.
This is mainly because $k$-Means calculates the relationship from a point to a cluster based on Euclidean distance to the centroids, rather than density.
This leads the algorithm to not be suitable in every situation. For example, if the points are distributed in ring structures, $k$-Means fails.

\paragraph{Size and density} The clusters generated by $k$-Means can vary in size a lot. By size we mean the number of points in a cluster, the cardinality.
$k$-Means has no upper or lower limit for cluster sizes. 
But $k$-Means has an implicit tendency to form clusters that cover regions of roughly similar spatial size.
This is due to the fact that $k$-Means tries to minimize the variances within a cluster.
But this does not mean that clusters contain a similar amount of points.
The only thing to control is the number of clusters $k$ to generate. 
\newline\newline
\emph{Example:} In a distribution, if one start cluster is large in space but barely populated with points and another is compact and dense, $k$-Means treats both clusters "equally" because it only minimizes distances to the center. This will result in clusters that cover similarly sized regions in space but can contain very different numbers of points.

\paragraph{Outliers} The $k$-Means algorithm is very sensitive to outliers. It assigns each point in the dataset to a cluster, even if it might be almost infinitely far away from the cluster centroid.

\section*{Impact on the R-Tree Index}
What $k$-Means does is it optimizes the sum of squared distances to centroids.
However, an R-Tree benefits from axis-aligned partitions with low overlap.
This leads to some mismatches.

\paragraph{Shape mismatch.}
As mentioned before, $k$-Means generates convex clusters (spherical), while R-Trees benefit from axis-aligned partitions (rectangles). This can lead to situations where the clusters produced by $k$-Means do not align well with the boundaries of the R-Tree nodes.
The axis-aligned MBR of a spherical cluster is much larger than the true cluster shape. Therefore, we have way more overlap with neighboring MBRs.

\paragraph{Capacity mismatch.}
$k$-Means has no lower or upper bound on the number of points per cluster.
Some clusters may become very small, while others become very big. We can just try to approximate a perfect cluster count $k$ (see Selection of Parameters).
\newline\newline
In short, $k$-Means gives a fast and stable warm start. 
Most of the index gains come from the later capacity-aware refinement that tightens box overlap and matches cluster sizes to page capacity.
